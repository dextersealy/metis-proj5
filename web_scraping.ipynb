{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import uuid\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import functions for creating nicely formatted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load 'utils.py'\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def pp_bold(str):\n",
    "    display(HTML('<b>{}</b>'.format(str)))\n",
    "\n",
    "\n",
    "def pp_listOflist(l):\n",
    "    display(HTML(\n",
    "        u'<table>{}</table>'.format(\n",
    "            u''.join(u'<tr>{}</tr>'.format(\n",
    "                u''.join(u'<td>{}</td>'.format(v) for v in sublist)) for sublist in l))))\n",
    "    \n",
    "\n",
    "def pp_dict(d, rows=None):\n",
    "    if not rows or rows >= len(d):\n",
    "        display(HTML(\n",
    "            u'<table>{}</table>'.format(\n",
    "                u''.join(u'<tr><td><b>{}</b></td><td>{}</td></tr>'.format(k, d[k]) for k in d))))\n",
    "    else:\n",
    "        nitems = len(d)\n",
    "        width = -(-nitems // rows)\n",
    "        i = 0\n",
    "        list_ = [[] for _ in range(rows)]\n",
    "        for _ in range(width):\n",
    "            for row in range(rows):\n",
    "                if i < nitems:\n",
    "                    k, v = d.items()[i]\n",
    "                    list_[row].extend(['<b>{}</b>'.format(k), v])\n",
    "                i += 1\n",
    "        pp_listOflist(list_)\n",
    "\n",
    "\n",
    "def pp_dictOflist(d):\n",
    "    display(HTML(\n",
    "        u'<table>{}</table>'.format(\n",
    "            u''.join(u'<tr><td><b>{}</b></td>{}</tr>'.format(k,\n",
    "                u''.join(u'<td>{}</td>'.format(v) for v in d[k])) for k in d.keys()))))\n",
    "    \n",
    "\n",
    "def pp_dfinfo(df, width=4):\n",
    "    ncols = len(df.columns)\n",
    "    width = min(width, ncols)\n",
    "    depth = -(-ncols // width)\n",
    "    i = 0\n",
    "    list_ = [[] for _ in range(depth)]\n",
    "    for _ in range(width):\n",
    "        for row in range(depth):\n",
    "            if i < ncols:\n",
    "                col = df.columns[i]\n",
    "                list_[row].extend(['<b>{}</b>'.format(col), df[col].count(), df.dtypes[i]])\n",
    "            i += 1\n",
    "\n",
    "    print('{} entries, {} columns'.format(len(df), ncols))\n",
    "    pp_listOflist(list_)\n",
    "\n",
    "\n",
    "def pp_counts(series, rows=1, caption=None):\n",
    "    if caption: pp_bold(caption)\n",
    "    list_ = [(k, '{:.4f}'.format(v)) for k, v in series.to_dict().items()] \n",
    "    dict_ = OrderedDict(sorted(list_, key=lambda x: x[0]))\n",
    "    pp_dict(dict_, rows)\n",
    "\n",
    "\n",
    "def pp_progress(s):\n",
    "    sys.stdout.write('\\r{}'.format(s))\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement simple web page cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "_cache = None\n",
    "_cache_dir = \"./cache\"\n",
    "_cache_index = os.path.join(_cache_dir, 'index.pkl')\n",
    "_cache_counter = 0\n",
    "\n",
    "def cache_init():\n",
    "    global _cache\n",
    "    if _cache == None:\n",
    "        if os.path.exists(_cache_index):\n",
    "            with open(_cache_index, 'rb') as fd:\n",
    "                _cache = pickle.load(fd)\n",
    "        else:\n",
    "            _cache = {}\n",
    "    return _cache\n",
    "\n",
    "def cache_get(key):\n",
    "    return cache_init().get(key, '')\n",
    "\n",
    "def cache_add(key, value):\n",
    "    global _cache_counter\n",
    "    cache = cache_init()\n",
    "    cache[key] = value\n",
    "    _cache_counter += 1\n",
    "    if _cache_counter % 100 == 0:\n",
    "        cache_commit()\n",
    "\n",
    "def cache_commit():\n",
    "    if not _cache == None:\n",
    "        with open(_cache_index, 'wb') as fd:\n",
    "            pickle.dump(_cache, fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StreetEasy aggresively blocks robots, so use we use Selenium with the Chrome driver (https://sites.google.com/a/chromium.org/chromedriver) to scrape listings. These functions download a page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "driver = None\n",
    "\n",
    "def get_driver():\n",
    "    global driver\n",
    "    if driver == None:\n",
    "        chromedriver = \"/home/dexter/bin/chromedriver\"\n",
    "        os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "        driver = webdriver.Chrome(chromedriver)\n",
    "    return driver\n",
    "\n",
    "def get_page(url, wait_for_element='pagination'):\n",
    "    \"\"\"Use Selenium driver to download a web page.\"\"\"\n",
    "\n",
    "    # Check if we have this page\n",
    "    \n",
    "    filename = cache_get(url)\n",
    "    if filename and os.path.exists(filename):\n",
    "        with open(filename, 'rb') as fd:\n",
    "            return fd.read().decode('utf-8')\n",
    "\n",
    "    # Otherwise, download the page ...\n",
    "    \n",
    "    get_driver().get(url)\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "    wait.until(EC.presence_of_element_located((By.CLASS_NAME, wait_for_element)))\n",
    "    html_doc = driver.page_source\n",
    "\n",
    "    # ... and cache it\n",
    "\n",
    "    global _cache_dir\n",
    "    if not os.path.isdir(_cache_dir):\n",
    "        os.mkdir(_cache_dir)\n",
    "        \n",
    "    if not filename:\n",
    "        filename = os.path.join(_cache_dir, uuid.uuid4().hex)\n",
    "\n",
    "    with open(filename, 'wb') as fd:\n",
    "        fd.write(html_doc.encode('utf-8'))\n",
    "\n",
    "    cache_add(url, filename)\n",
    "    \n",
    "    return html_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The StreetEasy 'for rent' section has multiple listings per page. This function parses one such listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "from geojson import Point\n",
    "\n",
    "def get_listing(element):\n",
    "    \"\"\"\n",
    "    Parse a listing from StreetEasy's 'for rent' page. It returns a dictionary with the\n",
    "    listing information, plus a set containing any unrecognized details.\n",
    "    \"\"\"\n",
    "\n",
    "    unknown = set()\n",
    "    listing = {\n",
    "        'created' : unicode(datetime.datetime.utcnow()).split('.')[0],\n",
    "        'bedrooms' : 0.0,\n",
    "        'furnished' : 0\n",
    "    }\n",
    "\n",
    "    # Get id, url and address\n",
    "    \n",
    "    details = element.find('div', class_='details row')\n",
    "    href = details.find('a', {'data-gtm-listing-id' : True})\n",
    "    listing['listing_id'] = int(href['data-gtm-listing-id'])\n",
    "    listing['url'] = unicode(href['href'].split('?')[0])\n",
    "    listing['street_address'] = unicode(href.string)\n",
    "\n",
    "    # Get GPS coordinates\n",
    "    \n",
    "    loc = element['se:map:point'].split(',')\n",
    "    if loc[0] and loc[1]:\n",
    "        listing['latitude'] = float(loc[0])\n",
    "        listing['longitude'] = float(loc[1])\n",
    "        listing['loc'] = Point((listing['longitude'], listing['latitude']))\n",
    "\n",
    "    # Get price\n",
    "    \n",
    "    price = element.find('span', class_='price')\n",
    "    if price:\n",
    "        listing['price'] = int(re.sub('[^0-9]', '', price.string))\n",
    "    \n",
    "    # Get size and whether furnished\n",
    "\n",
    "    for detail in details.find_all('span', {'class' : re.compile('.*?_detail_cell')}):\n",
    "        m = re.search('([0-9.,]+) ((bed|bath|ft))', detail.string)\n",
    "        if not m:\n",
    "            if 'Furnished' == detail.string:\n",
    "                listing['furnished'] = 1\n",
    "            else:\n",
    "                unknown.add(detail.string)\n",
    "        elif 'bed' == m.group(2):\n",
    "            listing['bedrooms'] = float(m.group(1))\n",
    "        elif 'bath' == m.group(2):\n",
    "            listing['bathrooms'] = float(m.group(1))\n",
    "        elif 'ft' == m.group(2):\n",
    "            listing['size'] = int(m.group(1).replace(',', ''))\n",
    "\n",
    "    return listing, unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the MongoDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>25709 listings</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient('ec2-34-198-246-43.compute-1.amazonaws.com', 27017)\n",
    "db = client.streeteasy\n",
    "collection = db.listings\n",
    "pp_bold('{} listings'.format(collection.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "URL_STREETEASY = 'http://streeteasy.com/for-rent/nyc?page={}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the pages and add new listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipping unpriced listing on page 665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 2169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unrecognized details: 7+ baths, 1+ bath, 4+ baths, 5+ baths, studio, 2+ baths, 3+ baths, 6+ baths\n"
     ]
    }
   ],
   "source": [
    "# Build set of existing listings\n",
    "\n",
    "seen = set([r['listing_id'] for r in collection.find({}, {'_id':0, 'listing_id':1})])\n",
    "\n",
    "# Bulk add new listings.\n",
    "\n",
    "all_unrecog = set() # track unrecognized listing details\n",
    "bulk = collection.initialize_ordered_bulk_op()\n",
    "for pageno in range(1, 2170):\n",
    "    pp_progress('Scraping page {}'.format(pageno))\n",
    "    \n",
    "    # Download the page\n",
    "\n",
    "    url = URL_STREETEASY.format(pageno)\n",
    "    html_doc = get_page(url)\n",
    "    soup = BeautifulSoup(html_doc, 'lxml')\n",
    "\n",
    "    # Extract the listings\n",
    "    \n",
    "    listing_elements = soup.find_all('div', class_='item')\n",
    "    for element in listing_elements:\n",
    "        \n",
    "        # Get next listing. If it has no price, punt.\n",
    "        \n",
    "        listing, unrecog = get_listing(element)\n",
    "        all_unrecog |= unrecog\n",
    "        if not 'price' in listing:\n",
    "            print('skipping unpriced listing on page {}'.format(pageno), file=sys.stderr)\n",
    "            continue\n",
    "    \n",
    "        # Add new listings\n",
    "\n",
    "        listing_id = listing['listing_id']\n",
    "        if not listing_id in seen:\n",
    "            seen.add(listing_id)\n",
    "            bulk.insert(listing)\n",
    "        \n",
    "    # Clean up\n",
    "    del soup, html_doc, listing_elements\n",
    "\n",
    "# Commit new listings\n",
    "\n",
    "try:\n",
    "    print('')\n",
    "    print(bulk.execute())\n",
    "    collection.create_index([(\"loc\", pymongo.GEOSPHERE)])\n",
    "except pymongo.errors.InvalidOperation as e:\n",
    "    if str(e) == 'No operations to execute':\n",
    "        pass\n",
    "    \n",
    "# Report unrecognized listing details\n",
    "\n",
    "print('unrecognized details: {}'.format(', '.join(all_unrecog)), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page 10770 of 25709, 2015923, http://www.streeteasy.com/building/7221-17-avenue-brooklyn/rental/2015923"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-28363c7e9589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mpp_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Getting page {} of {}, {}, {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlisting_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mhtml_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_for_element\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'details_info'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mhtml_doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-15b5c29b35fe>\u001b[0m in \u001b[0;36mget_page\u001b[0;34m(url, wait_for_element)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mget_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mwait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWebDriverWait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mwait\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muntil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpresence_of_element_located\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLASS_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_for_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mhtml_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dexter/anaconda2/lib/python2.7/site-packages/selenium/webdriver/support/wait.pyc\u001b[0m in \u001b[0;36muntil\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'screen'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mstacktrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stacktrace'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "removed_listings = [\n",
    "    2030929, 2029118, 2029115, 2028448, 2028443, 2028441, 2028423,\n",
    "    2028418, 2028416, 2027868, 2027264, 1978969, 1978963, 1978923,\n",
    "    1978905, 1978896, 1978890, 1978882, 1978883, 1978877, 1978878,\n",
    "    1978876, 1978873, 1978875, 1978856, 1978880, 1978816, 1978817,\n",
    "    1978796, 1978772, 1978738, 1978727, 1978724, 1978708, 1978681,\n",
    "    1978660, 1978611, 1978599, 1978598, 1978595, 1978566, 1978779,\n",
    "    1978705, 2024881, 2020587, 1978480, 1978417, 1978415, 1978411,\n",
    "    1978409, 1978410, 1978407, 1978395, 1978248, 1976817, 2026352,\n",
    "    1976530, 2016048,\n",
    "    ]\n",
    "listings = [r for r in collection.find({}, {'_id':0, 'listing_id':1, 'url':1})]\n",
    "try:\n",
    "    for i, listing in enumerate(listings):\n",
    "        url = 'http://www.streeteasy.com' + listing['url']\n",
    "        listing_id = listing['listing_id']\n",
    "        if not listing_id in removed_listings and not cache_get(url):\n",
    "            try:\n",
    "                pp_progress('Getting page {} of {}, {}, {}'.format(i + 1, len(listings), listing_id, url))\n",
    "                html_doc = get_page(url, wait_for_element='details_info')\n",
    "                del html_doc\n",
    "            except TimeoutException:\n",
    "                pass\n",
    "finally:\n",
    "    cache_commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del driver\n",
    "driver = None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "url = URL_STREETEASY.format(665)\n",
    "soup = BeautifulSoup(get_page(url), 'lxml')\n",
    "listing_elements = soup.find_all('div', class_='item')\n",
    "for element in listing_elements:\n",
    "    listing, _ = get_listing(element)\n",
    "    print(listing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
